# Haskell Spark

This is an attempt to have functionality similar to
[Apache Spark](http://spark.apache.org/) using
[Cloud Haskell](http://haskell-distributed.github.io/). 

**Apache Spark** implements many models for executing
RDD (Resilient Distributed Data). Theoretically possible, this
project implements only two models. One being *pure* model where RDD
is executed as if executing *native* haskell code. And second being
*distributed model* where same RDD can be executed over set of
*nodes*. 

## Dissecting Apache RDDs


## Static Pointer 

## Closure

## Context

# Execution Model 

## Pure Context

## Distribtued Context

### Work Distribution

### Shuffle

## References
* https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf

